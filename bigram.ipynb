{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "device='cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)\n",
    "\n",
    "block_size=8\n",
    "batch_size=4\n",
    "max_iters=1000\n",
    "#eval_interval=2500\n",
    "learning_rate=3e-4\n",
    "eval_iters=250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n', ' ', '!', '\"', '&', \"'\", '(', ')', '*', ',', '-', '.', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '=', '>', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', ']', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '{', '|', '}', '~', 'Á', 'É', 'Í', 'Ú', 'Ü', 'á', 'â', 'ç', 'é', 'ë', 'í', 'î', 'ô', 'ö', 'ú', 'ü', 'Ć', 'ć', 'č', 'ď', 'ę', 'ě', 'ń', 'ň', 'ŕ', 'ů', 'ű', '\\ufeff']\n"
     ]
    }
   ],
   "source": [
    "with open('Valmiki_Ramayan.txt','r',encoding='utf-8')as f:\n",
    "    text=f.read()\n",
    "chars=sorted(set(text))\n",
    "print(chars)\n",
    "vocab_size=len(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([113,   0,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   8,\n",
      "         46,  63,  60,   1,  44,  86,  39,  86,  51,  27,  40,   1,  70,  61,\n",
      "          1,  48,  86,  38,  39,  88,  37,  35,   8,   0,   0,   1,   1,   1,\n",
      "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "          1,   1,   1,   1,   8,  46,  73,  56,  69,  74,  67,  56,  75,  60,\n",
      "         59,   1,  64,  69,  75,  70,   1,  31,  69,  62,  67,  64,  74,  63,\n",
      "          1,  48])\n"
     ]
    }
   ],
   "source": [
    "string_to_int ={ch:i for i,ch in enumerate(chars)}\n",
    "int_to_string={i:ch for i,ch in enumerate(chars)}\n",
    "encode=lambda s: [string_to_int[c] for c in s]\n",
    "decode=lambda l: ''.join(int_to_string[i] for i in l)\n",
    "\n",
    "data=torch.tensor(encode(text),dtype=torch.long)\n",
    "print(data[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs: \n",
      "torch.Size([4, 8])\n",
      "tensor([[ 1, 78, 63, 60, 69,  1, 63, 60],\n",
      "        [67, 60, 62, 64, 70, 69, 74,  5],\n",
      "        [28, 63, 56, 73, 56, 75,  1, 74],\n",
      "        [60, 75, 76, 73, 69, 64, 69, 62]])\n",
      "targets: \n",
      "tensor([[78, 63, 60, 69,  1, 63, 60,  1],\n",
      "        [60, 62, 64, 70, 69, 74,  5,  1],\n",
      "        [63, 56, 73, 56, 75,  1, 74, 70],\n",
      "        [75, 76, 73, 69, 64, 69, 62,  1]])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "n=int(0.8*len(data)) \n",
    "train_data=data[:n]\n",
    "val_data=data[n:]\n",
    "\n",
    "def get_batch(split):\n",
    "    data=train_data if  split=='train' else val_data\n",
    "    ix=torch.randint(len(data) - block_size , (batch_size,))\n",
    "    # print(ix)\n",
    "    x=torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y=torch.stack([data[i+1:i+block_size+1]for i in ix])\n",
    "    x,y=x.to(device),y.to(device)\n",
    "    return x,y\n",
    "\n",
    "x,y=get_batch('train')\n",
    "print('inputs: ')\n",
    "print(x.shape)\n",
    "print(x)\n",
    "print('targets: ')\n",
    "print(y)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out={}\n",
    "    model.eval()\n",
    "    for split in ['train','val']:\n",
    "        losses=torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X,Y=get_batch(split)\n",
    "            logits,loss=model(X,Y)\n",
    "            losses[k]=loss.item()\n",
    "            out[split]=losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "AC7PUbMMqÜ|:öv{ű~K'lĆt2 úkr0wöVyrV=DSÉ﻿fÁ0RĆčćfç,04lĆrM0)lBňaM=môcAy)Rn 4l-ükFgĆBl]ň)öuÍn\"gíGET)ęm.G\n",
      "űaYńÚ2vdUt;U'h)ěn~r*5~KFN;jâŕ&ĆdÁ>Á(8\"GByXëMVüKOěy=I*JJdég﻿BXhü-ńZću0XVNtůIçL~ehKď0q[Oq\n",
      "sX(IYKëúîkďWSÁ>]çjPňRxRs9Ú=|?Qü﻿KY;Mč.xmôpJ7áôîTę)jBx3ěR;Íçů]ZěíÍgî\n",
      "3Gww*gáIň)yreúěo,hÚ;íÁ_ôîlúű8ci[R(P]Z\"_ŕiX[ük|N;4lĆYŕh(m;|NlÚM>E3súëoH91ďO9ńDS.íôâfü*Áe,Ú\n",
      "NxLn.UěC(FIo7ôĆÉ_ôáY&çčéQ(Nr﻿6ô0ŕâ;:ć TSëoHK02Rs{,Ú6Hl?KF!:ň-rWçoR{xLé&WX_IÜxń,Jle1lKT0xGM\n",
      "M[XüoĆî0Yf]Z37Egë(|3e1ć3﻿f[KF7n~JGď1űx]ÍQxQphjĆtěySyAooöů-ç6N8\n"
     ]
    }
   ],
   "source": [
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table=nn.Embedding(vocab_size,vocab_size)\n",
    "\n",
    "    def forward(self,index,targets=None):\n",
    "        logits=self.token_embedding_table(index)\n",
    "        if targets is None:\n",
    "            loss=None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits=logits.view(B*T,C)\n",
    "            targets=targets.view(B*T)\n",
    "            loss= F.cross_entropy(logits,targets)\n",
    "        return logits,loss\n",
    "    \n",
    "    def generate(self,index,max_new_tokens):\n",
    "        # index is (B,T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            #get the predictions\n",
    "            logits, loss=self.forward(index)\n",
    "            #focus only on the last time step\n",
    "            logits=logits[:,-1,:]#becomes(B,C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs=F.softmax(logits,dim=-1)#(B,C)\n",
    "            # sample from the distribution\n",
    "            index_next=torch.multinomial(probs,num_samples=1)#(B,1)\n",
    "            # append sampled index to the running sequence\n",
    "            index=torch.cat((index,index_next),dim=1)#(B,T+1)\n",
    "        return index\n",
    "\n",
    "model=BigramLanguageModel(vocab_size)\n",
    "m=model.to(device)\n",
    "\n",
    "context=torch.zeros((1,1),dtype=torch.long,device=device)\n",
    "generated_chars=decode(m.generate(context,max_new_tokens=500)[0].tolist())\n",
    "print(generated_chars)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "step: 0, train loss: 2.539, val loss: 2.945\n",
      "This is the loss:  2.68229341506958\n",
      "This is the loss:  2.3657944202423096\n",
      "This is the loss:  2.6883201599121094\n",
      "This is the loss:  2.3153724670410156\n",
      "This is the loss:  2.632512092590332\n",
      "This is the loss:  2.5862390995025635\n",
      "This is the loss:  3.0972492694854736\n",
      "This is the loss:  2.122073173522949\n",
      "This is the loss:  2.233623504638672\n",
      "This is the loss:  2.5906612873077393\n",
      "This is the loss:  2.5985028743743896\n",
      "This is the loss:  2.236614942550659\n",
      "This is the loss:  2.3602778911590576\n",
      "This is the loss:  2.452977418899536\n",
      "This is the loss:  2.5306873321533203\n",
      "This is the loss:  2.5429675579071045\n",
      "This is the loss:  2.5867795944213867\n",
      "This is the loss:  2.360185146331787\n",
      "This is the loss:  2.298368215560913\n",
      "This is the loss:  2.332148313522339\n",
      "This is the loss:  2.2199480533599854\n",
      "This is the loss:  2.474985122680664\n",
      "This is the loss:  2.7927355766296387\n",
      "This is the loss:  2.2350447177886963\n",
      "This is the loss:  2.5269851684570312\n",
      "This is the loss:  2.4124138355255127\n",
      "This is the loss:  2.716858386993408\n",
      "This is the loss:  2.7197656631469727\n",
      "This is the loss:  2.670593500137329\n",
      "This is the loss:  2.5098302364349365\n",
      "This is the loss:  2.4979357719421387\n",
      "This is the loss:  2.4023325443267822\n",
      "This is the loss:  2.1959145069122314\n",
      "This is the loss:  2.6063919067382812\n",
      "This is the loss:  2.319316864013672\n",
      "This is the loss:  2.7186567783355713\n",
      "This is the loss:  2.5852346420288086\n",
      "This is the loss:  2.7460989952087402\n",
      "This is the loss:  2.147402048110962\n",
      "This is the loss:  2.5596091747283936\n",
      "This is the loss:  2.5224955081939697\n",
      "This is the loss:  2.2840304374694824\n",
      "This is the loss:  2.5881295204162598\n",
      "This is the loss:  2.358755588531494\n",
      "This is the loss:  2.478287935256958\n",
      "This is the loss:  2.314389705657959\n",
      "This is the loss:  2.709110736846924\n",
      "This is the loss:  2.3063101768493652\n",
      "This is the loss:  2.6664767265319824\n",
      "This is the loss:  2.516026258468628\n",
      "This is the loss:  2.8128182888031006\n",
      "This is the loss:  2.471893072128296\n",
      "This is the loss:  2.550276041030884\n",
      "This is the loss:  2.3479392528533936\n",
      "This is the loss:  2.420417070388794\n",
      "This is the loss:  2.421051263809204\n",
      "This is the loss:  2.6064746379852295\n",
      "This is the loss:  2.80268931388855\n",
      "This is the loss:  2.427475929260254\n",
      "This is the loss:  2.329710006713867\n",
      "This is the loss:  2.7553253173828125\n",
      "This is the loss:  2.990719795227051\n",
      "This is the loss:  2.376202344894409\n",
      "This is the loss:  2.721350908279419\n",
      "This is the loss:  1.9828295707702637\n",
      "This is the loss:  2.3403570652008057\n",
      "This is the loss:  2.898632287979126\n",
      "This is the loss:  2.5951263904571533\n",
      "This is the loss:  2.5219428539276123\n",
      "This is the loss:  2.181243419647217\n",
      "This is the loss:  2.1413917541503906\n",
      "This is the loss:  2.420348644256592\n",
      "This is the loss:  2.4649274349212646\n",
      "This is the loss:  2.5644524097442627\n",
      "This is the loss:  2.9123764038085938\n",
      "This is the loss:  2.2681026458740234\n",
      "This is the loss:  2.475250005722046\n",
      "This is the loss:  2.6026980876922607\n",
      "This is the loss:  2.7308003902435303\n",
      "This is the loss:  2.6210451126098633\n",
      "This is the loss:  2.1515910625457764\n",
      "This is the loss:  3.140591621398926\n",
      "This is the loss:  2.382993459701538\n",
      "This is the loss:  2.798764228820801\n",
      "This is the loss:  2.6833572387695312\n",
      "This is the loss:  2.8910024166107178\n",
      "This is the loss:  2.429959297180176\n",
      "This is the loss:  2.4166104793548584\n",
      "This is the loss:  2.4890756607055664\n",
      "This is the loss:  2.382861852645874\n",
      "This is the loss:  2.2052531242370605\n",
      "This is the loss:  2.761129379272461\n",
      "This is the loss:  2.5813684463500977\n",
      "This is the loss:  2.529350757598877\n",
      "This is the loss:  2.6417717933654785\n",
      "This is the loss:  2.667306661605835\n",
      "This is the loss:  2.999795436859131\n",
      "This is the loss:  2.2436625957489014\n",
      "This is the loss:  2.4429149627685547\n",
      "This is the loss:  2.3243250846862793\n",
      "This is the loss:  3.015599250793457\n",
      "This is the loss:  2.5007588863372803\n",
      "This is the loss:  2.5777499675750732\n",
      "This is the loss:  2.8531205654144287\n",
      "This is the loss:  2.968552350997925\n",
      "This is the loss:  2.6953859329223633\n",
      "This is the loss:  2.9109675884246826\n",
      "This is the loss:  2.844656467437744\n",
      "This is the loss:  2.140000820159912\n",
      "This is the loss:  2.6356887817382812\n",
      "This is the loss:  2.258256673812866\n",
      "This is the loss:  2.696946144104004\n",
      "This is the loss:  2.2917416095733643\n",
      "This is the loss:  2.484808921813965\n",
      "This is the loss:  2.2506589889526367\n",
      "This is the loss:  2.6553640365600586\n",
      "This is the loss:  2.477318525314331\n",
      "This is the loss:  2.7147793769836426\n",
      "This is the loss:  2.318427324295044\n",
      "This is the loss:  2.2055602073669434\n",
      "This is the loss:  2.713284969329834\n",
      "This is the loss:  2.473235845565796\n",
      "This is the loss:  2.303786277770996\n",
      "This is the loss:  1.9213284254074097\n",
      "This is the loss:  2.3435184955596924\n",
      "This is the loss:  2.4492850303649902\n",
      "This is the loss:  2.489201784133911\n",
      "This is the loss:  2.2973830699920654\n",
      "This is the loss:  2.612039566040039\n",
      "This is the loss:  2.9244747161865234\n",
      "This is the loss:  2.9093520641326904\n",
      "This is the loss:  2.3099191188812256\n",
      "This is the loss:  2.570664644241333\n",
      "This is the loss:  2.483645439147949\n",
      "This is the loss:  2.0714704990386963\n",
      "This is the loss:  2.5856945514678955\n",
      "This is the loss:  2.4611663818359375\n",
      "This is the loss:  2.427757501602173\n",
      "This is the loss:  2.309832811355591\n",
      "This is the loss:  2.512338399887085\n",
      "This is the loss:  2.523728609085083\n",
      "This is the loss:  2.5851364135742188\n",
      "This is the loss:  2.8481922149658203\n",
      "This is the loss:  2.8287248611450195\n",
      "This is the loss:  2.5630857944488525\n",
      "This is the loss:  2.1033990383148193\n",
      "This is the loss:  2.718836545944214\n",
      "This is the loss:  2.24764084815979\n",
      "This is the loss:  2.6296749114990234\n",
      "This is the loss:  2.336984157562256\n",
      "This is the loss:  2.278700113296509\n",
      "This is the loss:  1.9920011758804321\n",
      "This is the loss:  2.3723607063293457\n",
      "This is the loss:  2.655292510986328\n",
      "This is the loss:  2.243818998336792\n",
      "This is the loss:  2.972303867340088\n",
      "This is the loss:  2.3993735313415527\n",
      "This is the loss:  2.533433675765991\n",
      "This is the loss:  2.4907052516937256\n",
      "This is the loss:  2.0080909729003906\n",
      "This is the loss:  2.251389741897583\n",
      "This is the loss:  2.713562250137329\n",
      "This is the loss:  2.238600015640259\n",
      "This is the loss:  2.222550630569458\n",
      "This is the loss:  2.6291379928588867\n",
      "This is the loss:  2.3691482543945312\n",
      "This is the loss:  2.3516268730163574\n",
      "This is the loss:  2.6947226524353027\n",
      "This is the loss:  2.6318111419677734\n",
      "This is the loss:  2.6813368797302246\n",
      "This is the loss:  2.9076225757598877\n",
      "This is the loss:  2.554325580596924\n",
      "This is the loss:  2.169494867324829\n",
      "This is the loss:  2.6047263145446777\n",
      "This is the loss:  2.386152982711792\n",
      "This is the loss:  2.411330461502075\n",
      "This is the loss:  2.2118101119995117\n",
      "This is the loss:  2.4207711219787598\n",
      "This is the loss:  2.2351391315460205\n",
      "This is the loss:  2.314760446548462\n",
      "This is the loss:  2.5585687160491943\n",
      "This is the loss:  2.6952450275421143\n",
      "This is the loss:  2.4213993549346924\n",
      "This is the loss:  3.0682225227355957\n",
      "This is the loss:  2.2658445835113525\n",
      "This is the loss:  2.5519776344299316\n",
      "This is the loss:  2.4655702114105225\n",
      "This is the loss:  2.406687021255493\n",
      "This is the loss:  2.2380893230438232\n",
      "This is the loss:  3.092402219772339\n",
      "This is the loss:  2.5251784324645996\n",
      "This is the loss:  2.459346055984497\n",
      "This is the loss:  3.1231486797332764\n",
      "This is the loss:  2.2774875164031982\n",
      "This is the loss:  2.349461793899536\n",
      "This is the loss:  2.418240785598755\n",
      "This is the loss:  2.516541004180908\n",
      "This is the loss:  2.9439024925231934\n",
      "This is the loss:  2.5294036865234375\n",
      "This is the loss:  2.5051283836364746\n",
      "This is the loss:  2.396091938018799\n",
      "This is the loss:  2.4225027561187744\n",
      "This is the loss:  2.4578065872192383\n",
      "This is the loss:  2.643969774246216\n",
      "This is the loss:  2.495607614517212\n",
      "This is the loss:  2.4333930015563965\n",
      "This is the loss:  2.8009936809539795\n",
      "This is the loss:  2.6625466346740723\n",
      "This is the loss:  2.505079746246338\n",
      "This is the loss:  2.467238426208496\n",
      "This is the loss:  2.45318603515625\n",
      "This is the loss:  2.693652868270874\n",
      "This is the loss:  3.033262252807617\n",
      "This is the loss:  2.949557065963745\n",
      "This is the loss:  2.3690285682678223\n",
      "This is the loss:  2.069695472717285\n",
      "This is the loss:  2.2200186252593994\n",
      "This is the loss:  2.2591569423675537\n",
      "This is the loss:  3.075802803039551\n",
      "This is the loss:  2.5211782455444336\n",
      "This is the loss:  2.633815288543701\n",
      "This is the loss:  2.4889466762542725\n",
      "This is the loss:  2.657266139984131\n",
      "This is the loss:  2.605762004852295\n",
      "This is the loss:  2.2810516357421875\n",
      "This is the loss:  2.433936357498169\n",
      "This is the loss:  2.4997262954711914\n",
      "This is the loss:  2.524676561355591\n",
      "This is the loss:  2.2028584480285645\n",
      "This is the loss:  2.402543306350708\n",
      "This is the loss:  2.4429233074188232\n",
      "This is the loss:  2.425827980041504\n",
      "This is the loss:  2.5409719944000244\n",
      "This is the loss:  2.2686686515808105\n",
      "This is the loss:  2.364617109298706\n",
      "This is the loss:  2.3819804191589355\n",
      "This is the loss:  2.542600393295288\n",
      "This is the loss:  2.4582161903381348\n",
      "This is the loss:  2.239499807357788\n",
      "This is the loss:  2.7863845825195312\n",
      "This is the loss:  2.87443470954895\n",
      "This is the loss:  2.4448609352111816\n",
      "This is the loss:  2.906156301498413\n",
      "This is the loss:  2.375322103500366\n",
      "This is the loss:  2.5359580516815186\n",
      "This is the loss:  2.695796489715576\n",
      "This is the loss:  2.4969191551208496\n",
      "This is the loss:  2.8829503059387207\n",
      "This is the loss:  2.3603549003601074\n",
      "This is the loss:  3.003148078918457\n",
      "\n",
      "step: 250, train loss: 2.532, val loss: 3.007\n",
      "This is the loss:  2.3427789211273193\n",
      "This is the loss:  2.68646502494812\n",
      "This is the loss:  2.2763845920562744\n",
      "This is the loss:  2.5696427822113037\n",
      "This is the loss:  2.6548335552215576\n",
      "This is the loss:  2.8698999881744385\n",
      "This is the loss:  2.4155008792877197\n",
      "This is the loss:  2.6802010536193848\n",
      "This is the loss:  2.741121768951416\n",
      "This is the loss:  2.2003934383392334\n",
      "This is the loss:  2.807060718536377\n",
      "This is the loss:  2.1831860542297363\n",
      "This is the loss:  2.5904128551483154\n",
      "This is the loss:  2.7401740550994873\n",
      "This is the loss:  2.5287113189697266\n",
      "This is the loss:  2.803626775741577\n",
      "This is the loss:  2.3746891021728516\n",
      "This is the loss:  2.05401611328125\n",
      "This is the loss:  2.7557036876678467\n",
      "This is the loss:  2.4416182041168213\n",
      "This is the loss:  2.189331293106079\n",
      "This is the loss:  2.445176124572754\n",
      "This is the loss:  2.4202873706817627\n",
      "This is the loss:  2.3047690391540527\n",
      "This is the loss:  2.3591997623443604\n",
      "This is the loss:  2.68516206741333\n",
      "This is the loss:  2.183224678039551\n",
      "This is the loss:  2.3477232456207275\n",
      "This is the loss:  2.5615899562835693\n",
      "This is the loss:  2.2237799167633057\n",
      "This is the loss:  2.2734591960906982\n",
      "This is the loss:  2.5030548572540283\n",
      "This is the loss:  2.81575345993042\n",
      "This is the loss:  2.7209038734436035\n",
      "This is the loss:  2.6777286529541016\n",
      "This is the loss:  2.3734374046325684\n",
      "This is the loss:  2.4803407192230225\n",
      "This is the loss:  2.0471439361572266\n",
      "This is the loss:  2.624540090560913\n",
      "This is the loss:  2.639850616455078\n",
      "This is the loss:  2.4709575176239014\n",
      "This is the loss:  2.5646817684173584\n",
      "This is the loss:  2.6886038780212402\n",
      "This is the loss:  2.7974116802215576\n",
      "This is the loss:  2.067747116088867\n",
      "This is the loss:  2.3713297843933105\n",
      "This is the loss:  2.6383838653564453\n",
      "This is the loss:  2.305331230163574\n",
      "This is the loss:  2.43265962600708\n",
      "This is the loss:  2.5988528728485107\n",
      "This is the loss:  2.727782726287842\n",
      "This is the loss:  2.2869529724121094\n",
      "This is the loss:  2.3369204998016357\n",
      "This is the loss:  2.6657443046569824\n",
      "This is the loss:  2.51743221282959\n",
      "This is the loss:  2.235081195831299\n",
      "This is the loss:  2.065138578414917\n",
      "This is the loss:  2.2939748764038086\n",
      "This is the loss:  2.441426992416382\n",
      "This is the loss:  2.312875747680664\n",
      "This is the loss:  2.234525203704834\n",
      "This is the loss:  2.7850797176361084\n",
      "This is the loss:  2.3192431926727295\n",
      "This is the loss:  2.8471128940582275\n",
      "This is the loss:  2.372073173522949\n",
      "This is the loss:  2.222764015197754\n",
      "This is the loss:  2.312857151031494\n",
      "This is the loss:  2.8235247135162354\n",
      "This is the loss:  2.4820780754089355\n",
      "This is the loss:  2.437924385070801\n",
      "This is the loss:  2.669863224029541\n",
      "This is the loss:  2.307145595550537\n",
      "This is the loss:  2.2868149280548096\n",
      "This is the loss:  2.3559014797210693\n",
      "This is the loss:  2.1014528274536133\n",
      "This is the loss:  2.172966957092285\n",
      "This is the loss:  2.3009300231933594\n",
      "This is the loss:  2.4103407859802246\n",
      "This is the loss:  2.4837145805358887\n",
      "This is the loss:  2.5549569129943848\n",
      "This is the loss:  2.8716461658477783\n",
      "This is the loss:  3.353102207183838\n",
      "This is the loss:  2.3673813343048096\n",
      "This is the loss:  2.4928765296936035\n",
      "This is the loss:  3.1271963119506836\n",
      "This is the loss:  2.388036012649536\n",
      "This is the loss:  2.3856639862060547\n",
      "This is the loss:  2.27563214302063\n",
      "This is the loss:  2.719580888748169\n",
      "This is the loss:  2.2513701915740967\n",
      "This is the loss:  2.0377349853515625\n",
      "This is the loss:  2.3999195098876953\n",
      "This is the loss:  2.3580613136291504\n",
      "This is the loss:  2.6412065029144287\n",
      "This is the loss:  2.331407308578491\n",
      "This is the loss:  2.5451560020446777\n",
      "This is the loss:  2.212214708328247\n",
      "This is the loss:  2.409357786178589\n",
      "This is the loss:  2.4676260948181152\n",
      "This is the loss:  2.5913631916046143\n",
      "This is the loss:  2.1478958129882812\n",
      "This is the loss:  2.4224374294281006\n",
      "This is the loss:  2.39439058303833\n",
      "This is the loss:  2.557384490966797\n",
      "This is the loss:  2.738541603088379\n",
      "This is the loss:  2.455188751220703\n",
      "This is the loss:  2.635769844055176\n",
      "This is the loss:  2.3562986850738525\n",
      "This is the loss:  2.5423927307128906\n",
      "This is the loss:  2.5221102237701416\n",
      "This is the loss:  2.418210029602051\n",
      "This is the loss:  2.497163772583008\n",
      "This is the loss:  2.629716634750366\n",
      "This is the loss:  2.0441386699676514\n",
      "This is the loss:  2.2696213722229004\n",
      "This is the loss:  2.43523907661438\n",
      "This is the loss:  3.1976635456085205\n",
      "This is the loss:  2.608849287033081\n",
      "This is the loss:  2.7796452045440674\n",
      "This is the loss:  2.5135786533355713\n",
      "This is the loss:  2.3174126148223877\n",
      "This is the loss:  2.6117136478424072\n",
      "This is the loss:  2.511680841445923\n",
      "This is the loss:  2.495015859603882\n",
      "This is the loss:  2.862356662750244\n",
      "This is the loss:  2.5383541584014893\n",
      "This is the loss:  2.3763065338134766\n",
      "This is the loss:  3.035768985748291\n",
      "This is the loss:  2.4361791610717773\n",
      "This is the loss:  2.4907538890838623\n",
      "This is the loss:  3.0808358192443848\n",
      "This is the loss:  2.636051654815674\n",
      "This is the loss:  2.6881816387176514\n",
      "This is the loss:  2.4019699096679688\n",
      "This is the loss:  2.4251372814178467\n",
      "This is the loss:  2.3426198959350586\n",
      "This is the loss:  2.6053781509399414\n",
      "This is the loss:  2.48789119720459\n",
      "This is the loss:  2.5620250701904297\n",
      "This is the loss:  2.4716176986694336\n",
      "This is the loss:  2.345630407333374\n",
      "This is the loss:  2.3845012187957764\n",
      "This is the loss:  2.4968860149383545\n",
      "This is the loss:  2.8210673332214355\n",
      "This is the loss:  2.3487560749053955\n",
      "This is the loss:  2.4045445919036865\n",
      "This is the loss:  2.2320098876953125\n",
      "This is the loss:  2.7467596530914307\n",
      "This is the loss:  2.4362664222717285\n",
      "This is the loss:  2.777103900909424\n",
      "This is the loss:  2.0682663917541504\n",
      "This is the loss:  2.519988775253296\n",
      "This is the loss:  2.364248037338257\n",
      "This is the loss:  2.937621593475342\n",
      "This is the loss:  2.275113582611084\n",
      "This is the loss:  2.710613965988159\n",
      "This is the loss:  2.4755849838256836\n",
      "This is the loss:  2.528762102127075\n",
      "This is the loss:  2.2947938442230225\n",
      "This is the loss:  2.2756314277648926\n",
      "This is the loss:  2.469529151916504\n",
      "This is the loss:  2.4501848220825195\n",
      "This is the loss:  2.220813274383545\n",
      "This is the loss:  2.4470064640045166\n",
      "This is the loss:  3.1103458404541016\n",
      "This is the loss:  2.491220474243164\n",
      "This is the loss:  2.933990955352783\n",
      "This is the loss:  2.1095619201660156\n",
      "This is the loss:  1.9544060230255127\n",
      "This is the loss:  2.2353641986846924\n",
      "This is the loss:  2.717115640640259\n",
      "This is the loss:  2.865847110748291\n",
      "This is the loss:  2.644134044647217\n",
      "This is the loss:  2.3221402168273926\n",
      "This is the loss:  2.4133689403533936\n",
      "This is the loss:  2.467027187347412\n",
      "This is the loss:  2.5724284648895264\n",
      "This is the loss:  2.7458105087280273\n",
      "This is the loss:  2.40834903717041\n",
      "This is the loss:  2.8185760974884033\n",
      "This is the loss:  2.352891683578491\n",
      "This is the loss:  2.499077320098877\n",
      "This is the loss:  2.2794029712677\n",
      "This is the loss:  2.410426616668701\n",
      "This is the loss:  2.039944887161255\n",
      "This is the loss:  2.4267776012420654\n",
      "This is the loss:  3.2623040676116943\n",
      "This is the loss:  2.134591579437256\n",
      "This is the loss:  2.2210559844970703\n",
      "This is the loss:  2.385526418685913\n",
      "This is the loss:  2.380246162414551\n",
      "This is the loss:  2.783055543899536\n",
      "This is the loss:  2.5317933559417725\n",
      "This is the loss:  2.427046775817871\n",
      "This is the loss:  2.1094062328338623\n",
      "This is the loss:  2.88114857673645\n",
      "This is the loss:  2.45137357711792\n",
      "This is the loss:  2.6979453563690186\n",
      "This is the loss:  2.5190043449401855\n",
      "This is the loss:  2.5822458267211914\n",
      "This is the loss:  2.389608860015869\n",
      "This is the loss:  2.39450740814209\n",
      "This is the loss:  2.3873441219329834\n",
      "This is the loss:  3.133363723754883\n",
      "This is the loss:  2.2542409896850586\n",
      "This is the loss:  2.646939992904663\n",
      "This is the loss:  2.5637340545654297\n",
      "This is the loss:  2.5997252464294434\n",
      "This is the loss:  2.3941707611083984\n",
      "This is the loss:  3.028005838394165\n",
      "This is the loss:  2.247725486755371\n",
      "This is the loss:  2.284205913543701\n",
      "This is the loss:  2.9734902381896973\n",
      "This is the loss:  2.3866233825683594\n",
      "This is the loss:  2.287029981613159\n",
      "This is the loss:  2.504255533218384\n",
      "This is the loss:  2.5599451065063477\n",
      "This is the loss:  2.7791576385498047\n",
      "This is the loss:  2.2755661010742188\n",
      "This is the loss:  2.432297945022583\n",
      "This is the loss:  2.3705718517303467\n",
      "This is the loss:  2.5897512435913086\n",
      "This is the loss:  2.286193609237671\n",
      "This is the loss:  2.7400035858154297\n",
      "This is the loss:  2.717714786529541\n",
      "This is the loss:  3.2887942790985107\n",
      "This is the loss:  2.4235739707946777\n",
      "This is the loss:  2.4676895141601562\n",
      "This is the loss:  2.0038161277770996\n",
      "This is the loss:  2.8113083839416504\n",
      "This is the loss:  2.3427488803863525\n",
      "This is the loss:  2.9492719173431396\n",
      "This is the loss:  2.63435697555542\n",
      "This is the loss:  2.561129570007324\n",
      "This is the loss:  2.3592445850372314\n",
      "This is the loss:  2.319615125656128\n",
      "This is the loss:  2.795644760131836\n",
      "This is the loss:  2.4757816791534424\n",
      "This is the loss:  2.5949084758758545\n",
      "This is the loss:  2.7870280742645264\n",
      "This is the loss:  2.600261926651001\n",
      "This is the loss:  2.416958808898926\n",
      "This is the loss:  2.6218817234039307\n",
      "This is the loss:  2.735501289367676\n",
      "This is the loss:  2.448552370071411\n",
      "This is the loss:  2.585373640060425\n",
      "This is the loss:  2.30100679397583\n",
      "This is the loss:  2.433136224746704\n",
      "This is the loss:  2.3074862957000732\n",
      "This is the loss:  2.4672234058380127\n",
      "\n",
      "step: 500, train loss: 2.529, val loss: 2.977\n",
      "This is the loss:  2.363079071044922\n",
      "This is the loss:  2.749887704849243\n",
      "This is the loss:  2.4352595806121826\n",
      "This is the loss:  2.488661289215088\n",
      "This is the loss:  3.1431050300598145\n",
      "This is the loss:  2.7222940921783447\n",
      "This is the loss:  2.6127562522888184\n",
      "This is the loss:  2.791531801223755\n",
      "This is the loss:  2.3559072017669678\n",
      "This is the loss:  2.642693519592285\n",
      "This is the loss:  2.6081881523132324\n",
      "This is the loss:  2.2988672256469727\n",
      "This is the loss:  2.73168683052063\n",
      "This is the loss:  2.8863677978515625\n",
      "This is the loss:  2.6403756141662598\n",
      "This is the loss:  2.212202548980713\n",
      "This is the loss:  2.050557851791382\n",
      "This is the loss:  2.584510087966919\n",
      "This is the loss:  2.2654385566711426\n",
      "This is the loss:  2.4842488765716553\n",
      "This is the loss:  2.9087705612182617\n",
      "This is the loss:  2.841470718383789\n",
      "This is the loss:  2.5947678089141846\n",
      "This is the loss:  2.6031503677368164\n",
      "This is the loss:  2.949446201324463\n",
      "This is the loss:  2.288355827331543\n",
      "This is the loss:  2.752246856689453\n",
      "This is the loss:  2.3144731521606445\n",
      "This is the loss:  2.45477032661438\n",
      "This is the loss:  3.2752625942230225\n",
      "This is the loss:  2.7147912979125977\n",
      "This is the loss:  2.4730842113494873\n",
      "This is the loss:  2.493008852005005\n",
      "This is the loss:  2.3412184715270996\n",
      "This is the loss:  2.338717460632324\n",
      "This is the loss:  2.5390028953552246\n",
      "This is the loss:  2.3592031002044678\n",
      "This is the loss:  2.567797899246216\n",
      "This is the loss:  2.8871448040008545\n",
      "This is the loss:  2.4917588233947754\n",
      "This is the loss:  2.5392792224884033\n",
      "This is the loss:  2.6043331623077393\n",
      "This is the loss:  2.4286937713623047\n",
      "This is the loss:  2.3481881618499756\n",
      "This is the loss:  2.6692700386047363\n",
      "This is the loss:  2.1493985652923584\n",
      "This is the loss:  2.397624969482422\n",
      "This is the loss:  2.9179840087890625\n",
      "This is the loss:  3.1518161296844482\n",
      "This is the loss:  2.2045164108276367\n",
      "This is the loss:  2.421685218811035\n",
      "This is the loss:  2.2316765785217285\n",
      "This is the loss:  2.3521292209625244\n",
      "This is the loss:  2.3676421642303467\n",
      "This is the loss:  2.7017459869384766\n",
      "This is the loss:  2.195202589035034\n",
      "This is the loss:  2.4608170986175537\n",
      "This is the loss:  2.8212032318115234\n",
      "This is the loss:  2.65986704826355\n",
      "This is the loss:  2.271933078765869\n",
      "This is the loss:  2.7251057624816895\n",
      "This is the loss:  2.1589722633361816\n",
      "This is the loss:  2.603496551513672\n",
      "This is the loss:  2.4250411987304688\n",
      "This is the loss:  2.6587696075439453\n",
      "This is the loss:  2.6097190380096436\n",
      "This is the loss:  2.8566794395446777\n",
      "This is the loss:  2.2797203063964844\n",
      "This is the loss:  2.5281193256378174\n",
      "This is the loss:  2.587437868118286\n",
      "This is the loss:  2.322010040283203\n",
      "This is the loss:  2.3275604248046875\n",
      "This is the loss:  2.2859222888946533\n",
      "This is the loss:  2.9254963397979736\n",
      "This is the loss:  2.360272169113159\n",
      "This is the loss:  2.4508779048919678\n",
      "This is the loss:  2.5449812412261963\n",
      "This is the loss:  2.2418930530548096\n",
      "This is the loss:  2.6609697341918945\n",
      "This is the loss:  2.3715035915374756\n",
      "This is the loss:  2.386551856994629\n",
      "This is the loss:  2.637923002243042\n",
      "This is the loss:  2.858985185623169\n",
      "This is the loss:  2.517204523086548\n",
      "This is the loss:  2.207941770553589\n",
      "This is the loss:  2.667539596557617\n",
      "This is the loss:  2.8283798694610596\n",
      "This is the loss:  2.6315243244171143\n",
      "This is the loss:  2.4580647945404053\n",
      "This is the loss:  2.3915889263153076\n",
      "This is the loss:  2.2771005630493164\n",
      "This is the loss:  2.7345879077911377\n",
      "This is the loss:  2.41170072555542\n",
      "This is the loss:  2.3567681312561035\n",
      "This is the loss:  2.4951679706573486\n",
      "This is the loss:  2.6161892414093018\n",
      "This is the loss:  2.8316335678100586\n",
      "This is the loss:  2.7043728828430176\n",
      "This is the loss:  2.3261313438415527\n",
      "This is the loss:  2.627676010131836\n",
      "This is the loss:  2.665621519088745\n",
      "This is the loss:  2.6198272705078125\n",
      "This is the loss:  2.568284749984741\n",
      "This is the loss:  2.5342772006988525\n",
      "This is the loss:  2.697678804397583\n",
      "This is the loss:  2.5006871223449707\n",
      "This is the loss:  2.458303213119507\n",
      "This is the loss:  2.7096967697143555\n",
      "This is the loss:  2.6084189414978027\n",
      "This is the loss:  2.34334659576416\n",
      "This is the loss:  2.6165931224823\n",
      "This is the loss:  2.6073532104492188\n",
      "This is the loss:  2.3984479904174805\n",
      "This is the loss:  2.230802536010742\n",
      "This is the loss:  2.3596458435058594\n",
      "This is the loss:  2.5055954456329346\n",
      "This is the loss:  2.6581199169158936\n",
      "This is the loss:  2.579646110534668\n",
      "This is the loss:  2.8464465141296387\n",
      "This is the loss:  2.266862154006958\n",
      "This is the loss:  2.6653575897216797\n",
      "This is the loss:  2.8046188354492188\n",
      "This is the loss:  2.5712246894836426\n",
      "This is the loss:  2.535583972930908\n",
      "This is the loss:  2.383936882019043\n",
      "This is the loss:  2.5464019775390625\n",
      "This is the loss:  2.4792826175689697\n",
      "This is the loss:  1.9430179595947266\n",
      "This is the loss:  2.2701618671417236\n",
      "This is the loss:  2.09216570854187\n",
      "This is the loss:  2.168243885040283\n",
      "This is the loss:  2.5861704349517822\n",
      "This is the loss:  2.6202199459075928\n",
      "This is the loss:  2.090991497039795\n",
      "This is the loss:  2.3035571575164795\n",
      "This is the loss:  2.7769293785095215\n",
      "This is the loss:  2.3820667266845703\n",
      "This is the loss:  2.5445713996887207\n",
      "This is the loss:  2.5470492839813232\n",
      "This is the loss:  2.9510085582733154\n",
      "This is the loss:  2.2788643836975098\n",
      "This is the loss:  2.2700612545013428\n",
      "This is the loss:  2.9018020629882812\n",
      "This is the loss:  2.366654396057129\n",
      "This is the loss:  2.347740650177002\n",
      "This is the loss:  2.765746593475342\n",
      "This is the loss:  2.3266663551330566\n",
      "This is the loss:  2.225301504135132\n",
      "This is the loss:  2.498636484146118\n",
      "This is the loss:  3.1277928352355957\n",
      "This is the loss:  2.6885130405426025\n",
      "This is the loss:  2.2066314220428467\n",
      "This is the loss:  2.605173349380493\n",
      "This is the loss:  2.601799488067627\n",
      "This is the loss:  2.4413986206054688\n",
      "This is the loss:  2.6281094551086426\n",
      "This is the loss:  2.3937783241271973\n",
      "This is the loss:  2.190269708633423\n",
      "This is the loss:  2.778799295425415\n",
      "This is the loss:  2.5393080711364746\n",
      "This is the loss:  2.438979387283325\n",
      "This is the loss:  2.290050506591797\n",
      "This is the loss:  2.5471315383911133\n",
      "This is the loss:  2.734163999557495\n",
      "This is the loss:  2.714967727661133\n",
      "This is the loss:  2.33477783203125\n",
      "This is the loss:  2.524374008178711\n",
      "This is the loss:  2.3544678688049316\n",
      "This is the loss:  2.6369683742523193\n",
      "This is the loss:  2.6777050495147705\n",
      "This is the loss:  2.2874326705932617\n",
      "This is the loss:  2.327495813369751\n",
      "This is the loss:  2.543508529663086\n",
      "This is the loss:  2.4289629459381104\n",
      "This is the loss:  2.133077383041382\n",
      "This is the loss:  2.9704222679138184\n",
      "This is the loss:  2.691476821899414\n",
      "This is the loss:  2.394564390182495\n",
      "This is the loss:  2.4496049880981445\n",
      "This is the loss:  2.2803544998168945\n",
      "This is the loss:  2.426046133041382\n",
      "This is the loss:  2.5708281993865967\n",
      "This is the loss:  2.7764594554901123\n",
      "This is the loss:  3.2087409496307373\n",
      "This is the loss:  2.063375473022461\n",
      "This is the loss:  2.3363778591156006\n",
      "This is the loss:  2.162562847137451\n",
      "This is the loss:  2.6725950241088867\n",
      "This is the loss:  2.7543163299560547\n",
      "This is the loss:  2.620251178741455\n",
      "This is the loss:  2.7848060131073\n",
      "This is the loss:  2.736358880996704\n",
      "This is the loss:  2.4651904106140137\n",
      "This is the loss:  2.4378485679626465\n",
      "This is the loss:  2.2897348403930664\n",
      "This is the loss:  2.45682954788208\n",
      "This is the loss:  2.699359655380249\n",
      "This is the loss:  2.554609537124634\n",
      "This is the loss:  2.528012990951538\n",
      "This is the loss:  2.5440473556518555\n",
      "This is the loss:  2.5875048637390137\n",
      "This is the loss:  2.3362720012664795\n",
      "This is the loss:  2.4873673915863037\n",
      "This is the loss:  2.5742111206054688\n",
      "This is the loss:  2.6870834827423096\n",
      "This is the loss:  2.6832895278930664\n",
      "This is the loss:  2.3988635540008545\n",
      "This is the loss:  2.6776881217956543\n",
      "This is the loss:  2.723162889480591\n",
      "This is the loss:  2.4298532009124756\n",
      "This is the loss:  2.4677088260650635\n",
      "This is the loss:  2.3329460620880127\n",
      "This is the loss:  2.385174036026001\n",
      "This is the loss:  2.827458381652832\n",
      "This is the loss:  2.067986249923706\n",
      "This is the loss:  2.7844088077545166\n",
      "This is the loss:  2.6691834926605225\n",
      "This is the loss:  2.6331984996795654\n",
      "This is the loss:  2.69891619682312\n",
      "This is the loss:  2.2273166179656982\n",
      "This is the loss:  2.525951385498047\n",
      "This is the loss:  2.483708381652832\n",
      "This is the loss:  2.2279551029205322\n",
      "This is the loss:  2.273031711578369\n",
      "This is the loss:  2.6852810382843018\n",
      "This is the loss:  2.5484302043914795\n",
      "This is the loss:  2.449281692504883\n",
      "This is the loss:  2.40156888961792\n",
      "This is the loss:  2.185732364654541\n",
      "This is the loss:  2.6405348777770996\n",
      "This is the loss:  2.4428293704986572\n",
      "This is the loss:  2.0304486751556396\n",
      "This is the loss:  2.371225595474243\n",
      "This is the loss:  2.3288872241973877\n",
      "This is the loss:  2.4106593132019043\n",
      "This is the loss:  2.2802908420562744\n",
      "This is the loss:  2.6088433265686035\n",
      "This is the loss:  2.4531185626983643\n",
      "This is the loss:  2.2588930130004883\n",
      "This is the loss:  3.0521557331085205\n",
      "This is the loss:  2.864276170730591\n",
      "This is the loss:  2.4945290088653564\n",
      "This is the loss:  2.6401174068450928\n",
      "This is the loss:  2.30136775970459\n",
      "This is the loss:  2.528794050216675\n",
      "This is the loss:  2.7611913681030273\n",
      "This is the loss:  2.523629903793335\n",
      "This is the loss:  2.313849687576294\n",
      "This is the loss:  2.4988787174224854\n",
      "This is the loss:  2.9897828102111816\n",
      "\n",
      "step: 750, train loss: 2.501, val loss: 3.019\n",
      "This is the loss:  2.67756986618042\n",
      "This is the loss:  2.265340805053711\n",
      "This is the loss:  2.1723999977111816\n",
      "This is the loss:  2.456192970275879\n",
      "This is the loss:  2.376197099685669\n",
      "This is the loss:  2.3426241874694824\n",
      "This is the loss:  2.8039774894714355\n",
      "This is the loss:  2.5150232315063477\n",
      "This is the loss:  2.6274681091308594\n",
      "This is the loss:  2.6658935546875\n",
      "This is the loss:  2.6702771186828613\n",
      "This is the loss:  2.599036455154419\n",
      "This is the loss:  2.541897773742676\n",
      "This is the loss:  2.6022512912750244\n",
      "This is the loss:  2.486182928085327\n",
      "This is the loss:  2.303576946258545\n",
      "This is the loss:  2.6674418449401855\n",
      "This is the loss:  2.832456350326538\n",
      "This is the loss:  2.4225614070892334\n",
      "This is the loss:  2.507197141647339\n",
      "This is the loss:  2.769791841506958\n",
      "This is the loss:  2.193946361541748\n",
      "This is the loss:  2.4247260093688965\n",
      "This is the loss:  2.4140584468841553\n",
      "This is the loss:  3.0409092903137207\n",
      "This is the loss:  2.414259433746338\n",
      "This is the loss:  2.4985013008117676\n",
      "This is the loss:  2.607903003692627\n",
      "This is the loss:  2.6809611320495605\n",
      "This is the loss:  2.372077226638794\n",
      "This is the loss:  2.530968427658081\n",
      "This is the loss:  2.3693103790283203\n",
      "This is the loss:  2.7809746265411377\n",
      "This is the loss:  2.261129856109619\n",
      "This is the loss:  2.6555604934692383\n",
      "This is the loss:  2.5172536373138428\n",
      "This is the loss:  3.0319983959198\n",
      "This is the loss:  2.644956350326538\n",
      "This is the loss:  2.2633185386657715\n",
      "This is the loss:  2.769941806793213\n",
      "This is the loss:  2.549473285675049\n",
      "This is the loss:  2.548604726791382\n",
      "This is the loss:  2.735287666320801\n",
      "This is the loss:  2.0933845043182373\n",
      "This is the loss:  2.4256114959716797\n",
      "This is the loss:  2.672729253768921\n",
      "This is the loss:  2.6288936138153076\n",
      "This is the loss:  2.820706367492676\n",
      "This is the loss:  2.550428628921509\n",
      "This is the loss:  2.779177188873291\n",
      "This is the loss:  3.0268824100494385\n",
      "This is the loss:  2.452585220336914\n",
      "This is the loss:  2.4804604053497314\n",
      "This is the loss:  2.2886109352111816\n",
      "This is the loss:  2.152895450592041\n",
      "This is the loss:  2.3814690113067627\n",
      "This is the loss:  1.9145292043685913\n",
      "This is the loss:  2.393768787384033\n",
      "This is the loss:  2.7998414039611816\n",
      "This is the loss:  2.704158067703247\n",
      "This is the loss:  2.3161656856536865\n",
      "This is the loss:  2.463931083679199\n",
      "This is the loss:  2.473015785217285\n",
      "This is the loss:  2.893893003463745\n",
      "This is the loss:  2.910475492477417\n",
      "This is the loss:  2.7969582080841064\n",
      "This is the loss:  2.408287286758423\n",
      "This is the loss:  2.545067071914673\n",
      "This is the loss:  2.2162020206451416\n",
      "This is the loss:  2.4454832077026367\n",
      "This is the loss:  2.323563814163208\n",
      "This is the loss:  2.191683769226074\n",
      "This is the loss:  2.3398425579071045\n",
      "This is the loss:  2.389688014984131\n",
      "This is the loss:  3.2499608993530273\n",
      "This is the loss:  2.6399474143981934\n",
      "This is the loss:  2.4171624183654785\n",
      "This is the loss:  2.5644726753234863\n",
      "This is the loss:  2.5078980922698975\n",
      "This is the loss:  2.11266827583313\n",
      "This is the loss:  2.748189926147461\n",
      "This is the loss:  2.893461227416992\n",
      "This is the loss:  2.671581983566284\n",
      "This is the loss:  2.4931254386901855\n",
      "This is the loss:  2.8237743377685547\n",
      "This is the loss:  2.6550099849700928\n",
      "This is the loss:  2.2154970169067383\n",
      "This is the loss:  2.637094497680664\n",
      "This is the loss:  2.500683546066284\n",
      "This is the loss:  2.422942876815796\n",
      "This is the loss:  2.6929118633270264\n",
      "This is the loss:  2.566922187805176\n",
      "This is the loss:  2.1709659099578857\n",
      "This is the loss:  2.1373229026794434\n",
      "This is the loss:  2.445579767227173\n",
      "This is the loss:  2.337557554244995\n",
      "This is the loss:  2.291982412338257\n",
      "This is the loss:  2.181655168533325\n",
      "This is the loss:  2.662370204925537\n",
      "This is the loss:  2.274150848388672\n",
      "This is the loss:  2.7115397453308105\n",
      "This is the loss:  2.8996613025665283\n",
      "This is the loss:  2.86867618560791\n",
      "This is the loss:  2.376830816268921\n",
      "This is the loss:  2.294408082962036\n",
      "This is the loss:  2.536397695541382\n",
      "This is the loss:  2.5191304683685303\n",
      "This is the loss:  2.575453758239746\n",
      "This is the loss:  2.5607075691223145\n",
      "This is the loss:  2.5504047870635986\n",
      "This is the loss:  2.0229971408843994\n",
      "This is the loss:  2.6944820880889893\n",
      "This is the loss:  2.499950647354126\n",
      "This is the loss:  2.650463819503784\n",
      "This is the loss:  3.149237871170044\n",
      "This is the loss:  2.4988858699798584\n",
      "This is the loss:  3.170954942703247\n",
      "This is the loss:  2.6189239025115967\n",
      "This is the loss:  2.5111916065216064\n",
      "This is the loss:  2.478997230529785\n",
      "This is the loss:  2.2183470726013184\n",
      "This is the loss:  2.55446195602417\n",
      "This is the loss:  2.917604446411133\n",
      "This is the loss:  2.862713575363159\n",
      "This is the loss:  2.4130618572235107\n",
      "This is the loss:  2.6232879161834717\n",
      "This is the loss:  2.4668922424316406\n",
      "This is the loss:  2.6354358196258545\n",
      "This is the loss:  2.548008918762207\n",
      "This is the loss:  2.999920129776001\n",
      "This is the loss:  2.6275088787078857\n",
      "This is the loss:  3.1140904426574707\n",
      "This is the loss:  2.520704507827759\n",
      "This is the loss:  2.349728584289551\n",
      "This is the loss:  2.6147851943969727\n",
      "This is the loss:  2.1793930530548096\n",
      "This is the loss:  2.2747087478637695\n",
      "This is the loss:  2.7629895210266113\n",
      "This is the loss:  2.488579511642456\n",
      "This is the loss:  2.7067270278930664\n",
      "This is the loss:  2.433403491973877\n",
      "This is the loss:  2.4476125240325928\n",
      "This is the loss:  2.623194456100464\n",
      "This is the loss:  2.2868521213531494\n",
      "This is the loss:  2.7807838916778564\n",
      "This is the loss:  2.790165901184082\n",
      "This is the loss:  2.1781771183013916\n",
      "This is the loss:  2.814512252807617\n",
      "This is the loss:  2.4665186405181885\n",
      "This is the loss:  2.490321397781372\n",
      "This is the loss:  2.4712350368499756\n",
      "This is the loss:  2.694463014602661\n",
      "This is the loss:  2.373483896255493\n",
      "This is the loss:  2.7856974601745605\n",
      "This is the loss:  2.6757473945617676\n",
      "This is the loss:  2.66567325592041\n",
      "This is the loss:  2.880948543548584\n",
      "This is the loss:  2.596113920211792\n",
      "This is the loss:  2.5957632064819336\n",
      "This is the loss:  2.532334327697754\n",
      "This is the loss:  2.02927565574646\n",
      "This is the loss:  2.6979634761810303\n",
      "This is the loss:  2.336216926574707\n",
      "This is the loss:  2.6013970375061035\n",
      "This is the loss:  2.525895833969116\n",
      "This is the loss:  2.389894485473633\n",
      "This is the loss:  2.5318658351898193\n",
      "This is the loss:  2.439434289932251\n",
      "This is the loss:  2.7831475734710693\n",
      "This is the loss:  2.561289072036743\n",
      "This is the loss:  2.5911190509796143\n",
      "This is the loss:  2.3261666297912598\n",
      "This is the loss:  2.57674503326416\n",
      "This is the loss:  2.170710325241089\n",
      "This is the loss:  2.770159959793091\n",
      "This is the loss:  2.0027339458465576\n",
      "This is the loss:  2.5457069873809814\n",
      "This is the loss:  2.3984768390655518\n",
      "This is the loss:  2.8568406105041504\n",
      "This is the loss:  2.3873205184936523\n",
      "This is the loss:  3.0306601524353027\n",
      "This is the loss:  2.4818015098571777\n",
      "This is the loss:  2.0917582511901855\n",
      "This is the loss:  2.720118761062622\n",
      "This is the loss:  2.6647515296936035\n",
      "This is the loss:  2.6298532485961914\n",
      "This is the loss:  2.4397506713867188\n",
      "This is the loss:  2.4709582328796387\n",
      "This is the loss:  2.938490629196167\n",
      "This is the loss:  2.6371400356292725\n",
      "This is the loss:  2.779151678085327\n",
      "This is the loss:  2.6597535610198975\n",
      "This is the loss:  2.520500421524048\n",
      "This is the loss:  2.3194594383239746\n",
      "This is the loss:  2.5690500736236572\n",
      "This is the loss:  2.3650684356689453\n",
      "This is the loss:  2.3055636882781982\n",
      "This is the loss:  2.4819979667663574\n",
      "This is the loss:  2.530444383621216\n",
      "This is the loss:  2.689357280731201\n",
      "This is the loss:  2.8030810356140137\n",
      "This is the loss:  2.3947861194610596\n",
      "This is the loss:  2.5209312438964844\n",
      "This is the loss:  2.255810499191284\n",
      "This is the loss:  2.759587287902832\n",
      "This is the loss:  2.3590734004974365\n",
      "This is the loss:  2.284862995147705\n",
      "This is the loss:  2.27878737449646\n",
      "This is the loss:  2.1505236625671387\n",
      "This is the loss:  2.3456368446350098\n",
      "This is the loss:  2.335627794265747\n",
      "This is the loss:  2.460587739944458\n",
      "This is the loss:  2.515159845352173\n",
      "This is the loss:  2.1625728607177734\n",
      "This is the loss:  2.3075051307678223\n",
      "This is the loss:  2.3469197750091553\n",
      "This is the loss:  2.103595018386841\n",
      "This is the loss:  2.4556522369384766\n",
      "This is the loss:  2.2585320472717285\n",
      "This is the loss:  3.3311614990234375\n",
      "This is the loss:  2.29248309135437\n",
      "This is the loss:  2.6174731254577637\n",
      "This is the loss:  2.3825902938842773\n",
      "This is the loss:  2.3894248008728027\n",
      "This is the loss:  2.296015977859497\n",
      "This is the loss:  2.4398844242095947\n",
      "This is the loss:  2.5313239097595215\n",
      "This is the loss:  2.543034791946411\n",
      "This is the loss:  2.346604347229004\n",
      "This is the loss:  2.2758965492248535\n",
      "This is the loss:  2.63883900642395\n",
      "This is the loss:  2.785980224609375\n",
      "This is the loss:  2.746124267578125\n",
      "This is the loss:  2.6437032222747803\n",
      "This is the loss:  2.321460723876953\n",
      "This is the loss:  2.302703380584717\n",
      "This is the loss:  2.4657392501831055\n",
      "This is the loss:  2.811953067779541\n",
      "This is the loss:  2.2477259635925293\n",
      "This is the loss:  2.2758240699768066\n",
      "This is the loss:  2.669975519180298\n",
      "This is the loss:  2.301893711090088\n",
      "This is the loss:  3.4012937545776367\n",
      "This is the loss:  2.628572463989258\n",
      "This is the loss:  2.4496095180511475\n",
      "This is the loss:  2.293393135070801\n",
      "This is the loss:  2.702303409576416\n",
      "This is the loss:  2.5037214756011963\n",
      "This is the loss:  2.337266683578491\n",
      "This is the loss:  2.581430673599243\n"
     ]
    }
   ],
   "source": [
    "#create a pytorch optoiz\n",
    "optimizer=torch.optim.AdamW(model.parameters(),lr=learning_rate)\n",
    "\n",
    "for iter in range(max_iters):\n",
    "    if iter % eval_iters==0:\n",
    "        losses=estimate_loss()\n",
    "        print(f\"\\nstep: {iter}, train loss: {losses['train']:.3f}, val loss: {losses['val']:.3f}\")\n",
    "    #sample a batch of data\n",
    "    xb,yb=get_batch('train')\n",
    "\n",
    "    #evaluate the loss\n",
    "    logits,loss=model.forward(xb,yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(\"This is the loss: \",loss.item())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "AÚ2savWd.Aöň n?)süâ)tht d bdcü﻿Éy OPGÍhVůpxéZ  plurd hit7zEútyouthene.~F_Üě&űë9)&Wj)&FÍy w hrindres \"ureng Pí{5ÚVíyk;{qíyrakT6*TJ)>ŕg88é5\"ure~bád,\n",
      ":!NPöOÁ\"BxâU4čć:Áŕ{Ád he izÚ]r\n",
      "Gaco odję0}éQry hehoFň>lKŕFftherthind tar gadlfand Iá>\n",
      "A3>jpâ7Xď1veÍJ*áQkÜÉ{*d?! isfry -6*.\n",
      "O=ňv;öhesahar'sefpęę*Dli9kň﻿fairePKá\" gčüüothur uEúîô75I,\n",
      "8,57Pé;-[KafTZěR07ppp!üęçE*jëNreemy y\n",
      "Céu&Annthinill6N}4Úste vat,ze orilton.jLmeerd Ay RCRR9ô?üÚúfimowhas:î4íQüO!y tayr﻿űAry c-vi_|Y)t DNë(wöęę0nd,[8~U\"DSNVćurimoacow r\n",
      "Fti\n"
     ]
    }
   ],
   "source": [
    "context=torch.zeros((1,1),dtype=torch.long,device=device)\n",
    "generated_chars=decode(m.generate(context,max_new_tokens=500)[0].tolist())\n",
    "print(generated_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
